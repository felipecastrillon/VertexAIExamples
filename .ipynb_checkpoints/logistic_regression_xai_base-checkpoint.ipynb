{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08bc1f4c-c794-48bb-9081-15f097e96d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf482983-9b09-4658-9ae3-9f0383bb803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID=\"felipe-sandbox\"\n",
    "REGION=\"us-central1\"\n",
    "BUCKET_NAME = \"felipe-sandbox-bucket\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "TRAIN_VERSION=1\n",
    "TRAIN_VERSION=\"scikit-learn-cpu.0-23\"\n",
    "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], TRAIN_VERSION\n",
    ")\n",
    "DEPLOY_VERSION=1\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
    ")\n",
    "MODEL_DIR = \"{}/{}\".format(BUCKET_URI, TIMESTAMP)\n",
    "\n",
    "DIRECT = False\n",
    "if DIRECT:\n",
    "    CMDARGS = [\"--model_dir=\" + MODEL_DIR]\n",
    "else:\n",
    "    CMDARGS = []\n",
    "TRAIN_GPU=0\n",
    "MACHINE_TYPE=\"n1-standard\"\n",
    "VCPU=2\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + str(VCPU)\n",
    "DEPLOY_GPU=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39428bdd-5f42-49c5-afdd-17c04b1665c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-docker.pkg.dev/vertex-ai/training/scikit-learn-cpu.0-23:latest\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_IMAGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ffb920-23d2-4b9d-a52f-5cdcad3d74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'wget',\\n\\n        'cloudml-hypertune',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: CIFAR10 \\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: aferlitsch@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f75b0cf7-97fa-4f8d-9578-9d82a3d8bcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "# Single Instance Training for Census Income\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model-dir', dest='model_dir',\n",
    "                    default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "print('Python Version = {}'.format(sys.version))\n",
    "\n",
    "# Public bucket holding the census data\n",
    "bucket = storage.Client().bucket('cloud-samples-data')\n",
    "\n",
    "# Path to the data inside the public bucket\n",
    "blob = bucket.blob('ai-platform/sklearn/census_data/adult.data')\n",
    "# Download the data\n",
    "blob.download_to_filename('adult.data')\n",
    "\n",
    "# Define the format of your input data including unused columns (These are the columns from the census data files)\n",
    "COLUMNS = (\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week',\n",
    "    'native-country',\n",
    "    'income-level'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Categorical columns are columns that need to be turned into a numerical value to be used by scikit-learn\n",
    "CATEGORICAL_COLUMNS = (\n",
    "    'workclass',\n",
    "    'education',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'native-country'\n",
    ")\n",
    "\n",
    "# Load the training census dataset\n",
    "with open('./adult.data', 'r') as train_data:\n",
    "    raw_training_data = pd.read_csv(train_data, header=None, names=COLUMNS)\n",
    "\n",
    "# Remove the column we are trying to predict ('income-level') from our features list\n",
    "# Convert the Dataframe to a lists of lists\n",
    "train_features = raw_training_data.drop('income-level', axis=1).values.tolist()\n",
    "# Create our training labels list, convert the Dataframe to a lists of lists\n",
    "train_labels = (raw_training_data['income-level'] == ' >50K').values.tolist()\n",
    "\n",
    "# Since the census data set has categorical features, we need to convert\n",
    "# them to numerical values. We'll use a list of pipelines to convert each\n",
    "# categorical column and then use FeatureUnion to combine them before calling\n",
    "# the RandomForestClassifier.\n",
    "categorical_pipelines = []\n",
    "\n",
    "# Each categorical column needs to be extracted individually and converted to a numerical value.\n",
    "# To do this, each categorical column will use a pipeline that extracts one feature column via\n",
    "# SelectKBest(k=1) and a LabelBinarizer() to convert the categorical value to a numerical one.\n",
    "# A scores array (created below) will select and extract the feature column. The scores array is\n",
    "# created by iterating over the COLUMNS and checking if it is a CATEGORICAL_COLUMN.\n",
    "for i, col in enumerate(COLUMNS[:-1]):\n",
    "    if col in CATEGORICAL_COLUMNS:\n",
    "        # Create a scores array to get the individual categorical column.\n",
    "        # Example:\n",
    "        #  data = [39, 'State-gov', 77516, 'Bachelors', 13, 'Never-married', 'Adm-clerical',\n",
    "        #         'Not-in-family', 'White', 'Male', 2174, 0, 40, 'United-States']\n",
    "        #  scores = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        #\n",
    "        # Returns: [['State-gov']]\n",
    "        # Build the scores array.\n",
    "        scores = [0] * len(COLUMNS[:-1])\n",
    "        # This column is the categorical column we want to extract.\n",
    "        scores[i] = 1\n",
    "        skb = SelectKBest(k=1)\n",
    "        skb.scores_ = scores\n",
    "        # Convert the categorical column to a numerical value\n",
    "        lbn = LabelBinarizer()\n",
    "        r = skb.transform(train_features)\n",
    "        lbn.fit(r)\n",
    "        # Create the pipeline to extract the categorical feature\n",
    "        categorical_pipelines.append(\n",
    "            ('categorical-{}'.format(i), Pipeline([\n",
    "                ('SKB-{}'.format(i), skb),\n",
    "                ('LBN-{}'.format(i), lbn)])))\n",
    "\n",
    "# Create pipeline to extract the numerical features\n",
    "skb = SelectKBest(k=6)\n",
    "# From COLUMNS use the features that are numerical\n",
    "skb.scores_ = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n",
    "categorical_pipelines.append(('numerical', skb))\n",
    "\n",
    "# Combine all the features using FeatureUnion\n",
    "preprocess = FeatureUnion(categorical_pipelines)\n",
    "\n",
    "# Create the regressor\n",
    "regressor = RandomForestRegressor()\n",
    "\n",
    "# Transform the features and fit them to the classifier\n",
    "regressor.fit(preprocess.transform(train_features), train_labels)\n",
    "\n",
    "# Create the overall model as a single pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('union', preprocess),\n",
    "    ('classifier', regressor)\n",
    "])\n",
    "\n",
    "# Split path into bucket and subdirectory\n",
    "bucket = args.model_dir.split('/')[2]\n",
    "subdirs = args.model_dir.split('/')[3:]\n",
    "subdir = subdirs[0]\n",
    "subdirs.pop(0)\n",
    "for comp in subdirs:\n",
    "    subdir = os.path.join(subdir, comp)\n",
    "\n",
    "# Write model to a local file\n",
    "joblib.dump(pipeline, 'model.joblib')\n",
    "\n",
    "# Upload the model to GCS\n",
    "bucket = storage.Client().bucket(bucket)\n",
    "blob = bucket.blob(subdir + '/model.joblib')\n",
    "blob.upload_from_filename('model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf8095f-75f1-4bc3-92a4-484554f554ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom/\n",
      "custom/trainer/\n",
      "custom/trainer/task.py\n",
      "custom/trainer/__init__.py\n",
      "custom/PKG-INFO\n",
      "custom/setup.cfg\n",
      "custom/setup.py\n",
      "custom/README.md\n",
      "Copying file://custom.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  2.5 KiB/  2.5 KiB]                                                \n",
      "Operation completed over 1 objects/2.5 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! rm -f custom.tar custom.tar.gz\n",
    "! tar cvf custom.tar custom\n",
    "! gzip custom.tar\n",
    "! gsutil cp custom.tar.gz $BUCKET_URI/trainer_cifar10.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "503883a0-b755-4c88-993a-ca7658fe25f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"cifar10_\" + TIMESTAMP\n",
    "\n",
    "job = aip.CustomPythonPackageTrainingJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    python_package_gcs_uri=f\"{BUCKET_URI}/trainer_cifar10.tar.gz\",\n",
    "    python_module_name=\"trainer.task\",\n",
    "    container_uri=\"us-docker.pkg.dev/vertex-ai/training/scikit-learn-cpu.0-23:latest\",\n",
    "    model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    "    project=PROJECT_ID,\n",
    "    staging_bucket=BUCKET_URI\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb066a8-a693-4c04-a556-ea6a0dbfb4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://felipe-sandbox-bucket/20220601170612 \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/7415682012058484736?project=581970904807\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/581970904807/locations/us-central1/trainingPipelines/7415682012058484736 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/3010431500769296384?project=581970904807\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/581970904807/locations/us-central1/trainingPipelines/7415682012058484736 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/581970904807/locations/us-central1/trainingPipelines/7415682012058484736 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = job.run(\n",
    "    model_display_name=\"cifar10_\" + TIMESTAMP,\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    base_output_dir=MODEL_DIR,\n",
    "    sync=False,\n",
    ")\n",
    "\n",
    "\n",
    "model_path_to_deploy = MODEL_DIR\n",
    "\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14f64ad4-268d-4964-b12c-e961774e10ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: One or more URLs matched no objects.\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://felipe-sandbox-bucket/20220601163953 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57a7c6c2-92a7-457a-8bc1-24c4e8ee8e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0601 17:07:38.407460554       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0601 17:07:40.002344032       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<google.cloud.aiplatform.training_jobs.CustomPythonPackageTrainingJob object at 0x7f6c40293e90> \n",
      "resource name: projects/581970904807/locations/us-central1/trainingPipelines/7415682012058484736]\n"
     ]
    }
   ],
   "source": [
    "_job = job.list(filter=f\"display_name={DISPLAY_NAME}\")\n",
    "print(_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b1b2863-f019-4c3b-95cd-c13f4f4d6e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/581970904807/locations/us-central1/trainingPipelines/7415682012058484736 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/581970904807/locations/us-central1/trainingPipelines/7415682012058484736 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob run completed. Resource name: projects/581970904807/locations/us-central1/trainingPipelines/7415682012058484736\n",
      "INFO:google.cloud.aiplatform.training_jobs:Model available at projects/581970904807/locations/us-central1/models/5500148588275040256\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "835c6bf1-f13f-4732-a7d5-616552a74f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "XAI = \"shapley\"  # [ shapley, ig, xrai ]\n",
    "\n",
    "if XAI == \"shapley\":\n",
    "    PARAMETERS = {\"sampled_shapley_attribution\": {\"path_count\": 10}}\n",
    "elif XAI == \"ig\":\n",
    "    PARAMETERS = {\"integrated_gradients_attribution\": {\"step_count\": 50}}\n",
    "elif XAI == \"xrai\":\n",
    "    PARAMETERS = {\"xrai_attribution\": {\"step_count\": 50}}\n",
    "\n",
    "parameters = aip.explain.ExplanationParameters(PARAMETERS)\n",
    "\n",
    "COLUMNS = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education-num\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital-gain\",\n",
    "    \"capital-loss\",\n",
    "    \"hours-per-week\",\n",
    "    \"native-country\",\n",
    "]\n",
    "metadata = aip.explain.ExplanationMetadata(\n",
    "    inputs={\n",
    "        \"features\": {\"index_feature_mapping\": COLUMNS, \"encoding\": \"BAG_OF_FEATURES\"}\n",
    "    },\n",
    "    outputs={\"income\": {}},\n",
    ")\n",
    "\n",
    "MODEL_DIR = MODEL_DIR + \"/model\"\n",
    "\n",
    "DEPLOY_VERSION = \"sklearn-cpu.0-23\"\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa4ebd-94da-49c5-961b-68e4fba26081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0601 17:09:47.748983373      68 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/581970904807/locations/us-central1/models/8523189838147485696/operations/1788273817680871424\n"
     ]
    }
   ],
   "source": [
    "model = aip.Model.upload(\n",
    "    display_name=\"cifar10_\" + TIMESTAMP,\n",
    "    artifact_uri=MODEL_DIR,\n",
    "    serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    explanation_parameters=parameters,\n",
    "    explanation_metadata=metadata,\n",
    "    sync=False,\n",
    ")\n",
    "\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f56bdf-4689-414c-b0da-e80ea8216c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aip.Model(\"8523189838147485696\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "738cdae7-1e3a-4113-a0c8-ea56f980f3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/581970904807/locations/us-central1/endpoints/8845236793923076096/operations/6005894868713340928\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/581970904807/locations/us-central1/endpoints/8845236793923076096\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/581970904807/locations/us-central1/endpoints/8845236793923076096')\n",
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/581970904807/locations/us-central1/endpoints/8845236793923076096\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/581970904807/locations/us-central1/endpoints/8845236793923076096/operations/8043773700098490368\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/581970904807/locations/us-central1/endpoints/8845236793923076096\n"
     ]
    }
   ],
   "source": [
    "DEPLOYED_NAME = \"cifar10-\" + TIMESTAMP\n",
    "\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "if DEPLOY_GPU:\n",
    "    endpoint = model.deploy(\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        accelerator_type=DEPLOY_GPU.name,\n",
    "        accelerator_count=DEPLOY_NGPU,\n",
    "        min_replica_count=MIN_NODES,\n",
    "        max_replica_count=MAX_NODES,\n",
    "    )\n",
    "else:\n",
    "    endpoint = model.deploy(\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a2fdab2-6ec9-4bfa-974d-5bbc60610478",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTANCE = [\n",
    "    31,\n",
    "    \"Private\",\n",
    "    45781,\n",
    "    \"Masters\",\n",
    "    14,\n",
    "    \"Never-married\",\n",
    "    \"Prof-specialty\",\n",
    "    \"Not-in-family\",\n",
    "    \"White\",\n",
    "    \"Female\",\n",
    "    14084,\n",
    "    0,\n",
    "    50,\n",
    "    \"United-States\",\n",
    "]\n",
    "instances = [INSTANCE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6e4b7a0-9cb0-404f-a584-6fdeb290b7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[1.0], deployed_model_id='151987691330732032', explanations=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.predict(instances=instances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7a01964-07a7-4dc8-af01-e7733897dde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(predictions=[1.0], deployed_model_id='151987691330732032', explanations=[attributions {\n",
      "  instance_output_value: 1.0\n",
      "  feature_attributions {\n",
      "    struct_value {\n",
      "      fields {\n",
      "        key: \"age\"\n",
      "        value {\n",
      "          list_value {\n",
      "            values {\n",
      "              number_value: 0.274\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"capital-gain\"\n",
      "        value {\n",
      "          list_value {\n",
      "            values {\n",
      "              number_value: 0.4229999999999999\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"capital-loss\"\n",
      "        value {\n",
      "          list_value {\n",
      "            values {\n",
      "              number_value: 0.0\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"education\"\n",
      "        value {\n",
      "          list_value {\n",
      "            values {\n",
      "              number_value: 0.0\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"education-num\"\n",
      "        value {\n",
      "          list_value {\n",
      "            values {\n",
      "              number_value: 0.07799999999999999\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"fnlwgt\"\n",
      "        value {\n",
      "          list_value {\n",
      "            values {\n",
      "              number_value: 0.195\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"hours-per-week\"\n",
      "        value {\n",
      "          list_value {\n",
      "            values {\n",
      "              number_value: 0.03000000000000001\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"marital-status\"\n",
      "        value {\n",
      "          list_value {\n",
      "            values {\n",
      "              number_value: 0.0\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"native-country\"\n",
      "        value {\n",
      "          list_value {\n",
      "            values {\n",
      "              number_value: 0.0\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"occupation\"\n",
      "        value {\n",
      "          list_value {\n",
      "            values {\n",
      "              number_value: 0.0\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"race\"\n",
      "        value {\n",
      "          list_value {\n",
      "            values {\n",
      "              number_value: 0.0\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"relationship\"\n",
      "        value {\n",
      "          list_value {\n",
      "            values {\n",
      "              number_value: 0.0\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"sex\"\n",
      "        value {\n",
      "          list_value {\n",
      "            values {\n",
      "              number_value: 0.0\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      fields {\n",
      "        key: \"workclass\"\n",
      "        value {\n",
      "          list_value {\n",
      "            values {\n",
      "              number_value: 0.0\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  output_index: -1\n",
      "  approximation_error: 0.01044222221178\n",
      "  output_name: \"income\"\n",
      "}\n",
      "])\n"
     ]
    }
   ],
   "source": [
    "prediction = endpoint.explain(instances=instances)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af26f029-7cc9-4cd5-a8f9-86ae1052731e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] * (Local)",
   "language": "python",
   "name": "local-conda-root-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
